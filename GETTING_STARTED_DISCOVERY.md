# Getting Started with Novel Discoveries

Quick guide to executing the practical discovery plan with your validated system.

---

## Prerequisites

```bash
# Install required packages
pip install lightkurve astroquery schedule tqdm

# Verify installation
python -c "import lightkurve; print('‚úì lightkurve installed')"
python -c "import astroquery; print('‚úì astroquery installed')"
```

---

## Discovery Path 1: Rescue Kepler Uncertain Candidates

**Goal:** Confirm 100-500 new exoplanets from uncertain candidate pile  
**Timeline:** 1-2 weeks  
**Cost:** $0 (uses Modulus-Small)

### Step 1: Fetch Candidate Data (30-60 minutes)

```bash
cd backend
python scripts/fetch_kepler_uncertain.py
```

**What this does:**
- Queries NASA Exoplanet Archive for "CANDIDATE" status KOIs
- Filters for Earth-like, habitable zone targets
- Downloads 100 light curves for analysis
- Saves metadata and summary statistics

**Expected output:**
```
‚úì Downloaded 100 candidate KOIs
‚úì Saved metadata to backend/data/kepler_candidates/candidate_metadata.csv
‚úì Saved summary to backend/data/kepler_candidates/candidate_summary.txt
```

### Step 2: Re-analyze with Physics Filtering (1-2 hours)

```bash
python scripts/rescue_kepler_candidates.py
```

**What this does:**
- Runs BLS period search on each light curve
- Validates transit physics with Modulus
- Applies RL triage policy
- Classifies as CONFIRMED / NEEDS_REVIEW / FALSE_POSITIVE

**Expected output:**
```
‚úÖ CONFIRMED planets: 10-30 (10-30%)
üîç NEEDS REVIEW: 20-40 (20-40%)
‚ùå FALSE POSITIVES: 30-60 (30-60%)
‚≠ê HIGH-CONFIDENCE confirmations: 5-15
üåç HABITABLE ZONE candidates: 2-10
```

### Step 3: Review Results

```bash
# View top confirmations
cat backend/data/rescued_candidates/rescue_summary_*.txt

# Open results CSV
python -c "import pandas as pd; df = pd.read_csv('backend/data/rescued_candidates/rescue_results_*.csv'); print(df[df['status'] == 'CONFIRMED'].head(20))"
```

### Step 4: Prepare Publication

**Target Journal:** Astronomical Journal (AJ) or MNRAS  
**Paper Title:** "Rescuing Kepler Exoplanet Candidates with Physics-Informed Machine Learning: XXX New Planet Confirmations"

**To include:**
1. List of confirmed planets (Table 1)
2. Orbital parameters (Table 2)
3. Comparison with original KOI parameters
4. False positive rejection statistics
5. Method validation on known planets

**Timeline:** 2-4 weeks to manuscript submission

---

## Discovery Path 2: Rapid JWST Analysis

**Goal:** First-to-publish on newly released JWST spectra  
**Timeline:** Continuous (24-hour turnaround)  
**Cost:** $0 (uses Modulus-Small)

### Step 1: Start Monitoring Service

```bash
cd backend
python scripts/monitor_jwst_releases.py &
```

**What this does:**
- Checks MAST archive every 6 hours for new JWST spectra
- Automatically downloads new transmission spectroscopy observations
- Runs biosignature analysis within 1 hour of download
- Generates alert if biosignature score >0.5

**Monitoring output:**
```
‚úì Check interval: Every 6 hours
‚úì Lookback window: 7 days
Service starting...
```

### Step 2: Review Alerts (When Generated)

```bash
# Check for new alerts
ls backend/data/jwst_new/ALERT_*.txt

# View alert details
cat backend/data/jwst_new/ALERT_jw*.txt
```

**If biosignature detected:**
1. Manual review of spectrum
2. Compare with published results (if any)
3. Prepare rapid-communication paper
4. Submit to ApJ Letters within 48 hours

### Step 3: Stop Monitoring (When Done)

```bash
# Find process ID
ps aux | grep monitor_jwst

# Stop service
kill <PID>
```

---

## Discovery Path 3: Extended Biosignature Pairs

**Goal:** Detect novel biosignature combinations  
**Timeline:** 1-2 weeks  
**Cost:** $0 (uses existing data + Modulus-Small)

### Biosignature pairs already implemented:
‚úÖ O‚ÇÇ + CH‚ÇÑ (Earth-like)  
‚úÖ O‚ÇÇ + O‚ÇÉ (photochemical)  

### To add more pairs:

Edit `backend/core/biosignatures.py`:

```python
# Add to biosignature_molecules dict
'N2O': {'weight': 0.7, 'primary_wavelength_um': 7.8, 'biosignature_strength': 'strong'},
'NH3': {'weight': 0.5, 'primary_wavelength_um': 10.5, 'biosignature_strength': 'moderate'},
'CH3Cl': {'weight': 0.6, 'primary_wavelength_um': 3.4, 'biosignature_strength': 'moderate'},

# Add to disequilibrium pairs in _check_disequilibrium_pairs()
('N2O', 'CH4'): {'score': 0.75, 'explanation': 'Agricultural/microbial biosignature'},
('NH3', 'CH4'): {'score': 0.65, 'explanation': 'Early Earth analog'},
```

Then re-run discovery scan:

```bash
python run_discovery_scan.py
```

**Publication target:** *Astrobiology* journal

---

## Discovery Path 4: False-Positive Catalog

**Goal:** Create community resource of validated/rejected KOIs  
**Timeline:** 2-3 weeks  
**Cost:** $0

### Step 1: Bulk Analysis

```bash
cd backend
python scripts/fetch_kepler_uncertain.py  # If not done already
python scripts/rescue_kepler_candidates.py
```

### Step 2: Generate Catalog

```python
# Script to generate clean catalog
import pandas as pd

# Load results
results = pd.read_csv('backend/data/rescued_candidates/rescue_results_*.csv')

# Create catalog
catalog = results[['koi_id', 'status', 'confidence', 'snr', 'rejection_reason']].copy()
catalog['disposition'] = catalog['status'].map({
    'CONFIRMED': 'PLANET',
    'FALSE_POSITIVE': 'FALSE_POSITIVE',
    'NEEDS_REVIEW': 'CANDIDATE'
})

# Save
catalog.to_csv('kepler_fp_catalog_v1.0.csv', index=False)
print(f"Catalog generated with {len(catalog)} entries")
```

### Step 3: Validate Catalog

```python
# Compare with known confirmed planets
confirmed = pd.read_csv('https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=select+*+from+ps&format=csv')

# Calculate precision/recall
# (See DISCOVERY_PLAN.md for full code)
```

**Publication target:** AJ or PASP

---

## Quick Start: Run Everything

```bash
# Terminal 1: Fetch and analyze Kepler candidates
cd backend
python scripts/fetch_kepler_uncertain.py
python scripts/rescue_kepler_candidates.py

# Terminal 2: Start JWST monitoring (background)
python scripts/monitor_jwst_releases.py &

# Terminal 3: Monitor for alerts
watch -n 3600 'ls backend/data/jwst_new/ALERT_*.txt'
```

---

## Expected Discoveries (6 months)

| Discovery Type | Probability | Impact |
|----------------|-------------|--------|
| **100-300 confirmed Kepler planets** | High (90%) | 1-2 papers in AJ/MNRAS |
| **False-positive catalog** | Very High (95%) | Community resource, high citations |
| **Extended biosignature methodology** | High (85%) | Paper in Astrobiology |
| **Rapid JWST analysis** | Medium (50%) | If timing aligns, ApJ Letters |
| **Novel biosignature detection** | Low (10%) | Requires perfect target + Medium upgrade |

---

## Troubleshooting

### Issue: `lightkurve` download fails
**Solution:** Check NASA MAST status at https://mast.stsci.edu/

### Issue: "No metadata found"
**Solution:** Run `fetch_kepler_uncertain.py` first

### Issue: Modulus API timeout
**Solution:** System uses local mode by default (no API needed)

### Issue: Out of disk space
**Solution:** Each light curve is ~1MB, 100 candidates = ~100MB

---

## Next Steps After First Discoveries

1. ‚úÖ Publish methods paper (PASP)
2. ‚úÖ Publish first discovery paper (AJ/MNRAS)
3. üîÑ Apply for JWST observing time (target rescued candidates)
4. üîÑ Upgrade to Modulus-Medium for 2D analysis
5. üîÑ Scale to 1000+ candidate analysis

---

## Support & Questions

- üìñ Full details: See `DISCOVERY_PLAN.md`
- üî¨ Validation report: See `CONTROL_VALIDATION_REPORT.md`
- üìä System status: See `PRODUCTION_STATUS.md`
- üêõ Issues: GitHub Issues

---

**Last Updated:** October 3, 2025  
**Status:** ‚úÖ Ready to Execute


